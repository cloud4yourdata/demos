{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{\"conf\": {\"spark.jars.packages\": \"com.databricks:spark-avro_2.11:3.1.0\" }}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'pyspark3', u'conf': {u'spark.jars.packages': u'com.databricks:spark-avro_2.11:3.1.0'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nspark = SparkSession\\\n    .builder\\\n    .appName(\"EventDataCapture\") \\\n    .config('spark.hadoop.avro.mapred.ignore.inputs.without.extension', 'false') \\\n    .getOrCreate()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1513931680049_0004</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-sparkl.bbtczf3hypjufmfoyzoit54yrd.fx.internal.cloudapp.net:8088/proxy/application_1513931680049_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.6:30060/node/containerlogs/container_1513931680049_0004_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"source": "//Data from ADLS -> result of Event Data Capture", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "avroFiles = \"adl://adlssparklab.azuredatalakestore.net/mySamples/edcAvro/*.avro\"", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 4, "cell_type": "code", "source": "avroDf = spark.read.format(\"com.databricks.spark.avro\").load(avroFiles)\njsonRdd = avroDf.select(avroDf.Body.cast(\"string\")).rdd.map(lambda x: x[0])", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "data = spark.read.json(jsonRdd)\nprint(data)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "DataFrame[humidity: bigint, id: bigint, temperature: bigint, timestamp: string, waterLevel: bigint]"}], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "data.createOrReplaceTempView(\"edc\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 7, "cell_type": "code", "source": "res = spark.sql(\"SELECT id,max(waterLevel) FROM edc group by id order by id\")", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "res.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---+---------------+\n| id|max(waterLevel)|\n+---+---------------+\n|  2|             18|\n|  4|             57|\n|  6|             98|\n|  8|             59|\n| 10|            113|\n+---+---------------+"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}